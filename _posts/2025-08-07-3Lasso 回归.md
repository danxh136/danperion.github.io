---
layout: article
title: "回归模型-3Lasso 回归"
mathjax: true
mathjax_autoNumber: true
mermaid: true
chart: true
key: a-key-name
---

#### 3 Lasso 回归 

| 看模型的范式 |              Lasso 回归              |
| :----------: | :----------------------------------: |
|  问题是什么  |               回归拟合               |
|  模型是什么  |               线性模型               |
|   优化指标   | 最小二乘损失的基础上添加 L1 正则化项 |
|   求解方法   |              坐标下降法              |
|   评价模型   |                                      |

[一篇入门之-Lasso回归模型原理与实现（含代码）-老饼讲解](https://www.bbbdata.com/text/597)

[线性回归大结局(岭(Ridge)、 Lasso回归原理、公式推导)，你想要的这里都有 - 一无是处的研究僧 - 博客园](https://www.cnblogs.com/Chang-LeHung/p/16732520.html)

B站

首先，lasso与岭回归都是正则化目标函数。但是，与岭回归不同，Lasso 使用11正则化，这会带来特征选择的特性——将不重要特征的系数压缩至零，也就是生成稀疏解。
Lasso 在最小二乘损失的基础上添加 L1 正则化项(系数绝对值之和),其目标函数为：

$$
\begin{matrix}\text{Q=}&\min_\beta\left\{\|\mathbf{y}-\mathbf{X}\boldsymbol{\beta}\|_2^2+\lambda\|\boldsymbol{\beta}\|_1\right\}\end{matrix}
$$

其中$λ$>0是正则化参数，$\|\beta\|_1=\sum_{j=1}^p|\beta_j|$是L1正则化项。我们的目标是求解参数向量$β$,使得$Q$的值最小。

由于L1正则项 $∥β∥_1=\sum_{j=1}^{n}|\beta_j|$ 不可导，故不能用常规求矩阵偏导的方式来计算β向量，一般采用坐标下降法。在固定其他参数的情况下，对每个系数βj依次优化。具体方法如下：

令 $\mathbf{r}^{(j)}=\mathbf{y}-\sum_{k\neq j}\mathbf{x}_k\beta_k$ 其中 $\mathbf{x}_j$ 是数据矩阵 $\mathbf{X}$ 的第j列；

令 $\rho_j=\mathbf{x}_j^\top\mathbf{r}^{(j)}$，$z_j=\|\mathbf{x}_j\|_2^2$

β的解由软阈值函数给出：$\hat{\beta}_j=\frac{1}{z_j}S(\rho_j,\lambda/2)$

其中软阈值算子 $S(z,\gamma)$ 表示为：$S(z,\gamma)=\begin{cases}z-\gamma & \text{if } z>\gamma \\ 0 & \text{if } |z|\leq\gamma \\ z+\gamma & \text{if } z<-\gamma\end{cases}$

初始设置 $\beta_1=0$，然后用 $\beta_1$ 去计算剩余 $\beta$，同样的其余 $\beta_k$ 也可用来更新 $\beta_1$，这样反复迭代直至收敛。