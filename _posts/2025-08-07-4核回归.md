---
layout: article
title: "回归模型-4核回归"
---

#### 4 核回归

| 看模型的范式 |                核回归                |
| :----------: | :----------------------------------: |
|  问题是什么  |               回归问题               |
|  模型是什么  |               线性模型               |
|   优化指标   | 间隔最大化（最大化支持面之间的距离） |
|   求解方法   |      （序列最小最优化算法）smo       |
|   评价模型   |                                      |

[统计学基础（核回归） | SIRLIS](https://sirlis.cn/posts/statistics-kernel-regression/)

[Kernel Regression 核回归 详细讲解 - 编程宝典](https://www.bianchengbaodian.com/article/e78ee4714ae834a7d1795908af4d14b8.html)

[资料\kernel.pdf](file:///C:/Users/单小航/Desktop/研0-AI方向/资料/kernel.pdf)

核回归（Kernel Regression）是一种**非参数回归方法**，其核心思想是通过“局部加权平均”估计因变量 Y 对自变量 X 的条件期望 E[Y∣X=x]，无需假设具体的函数形式（如线性、多项式等），适用于捕捉复杂的非线性关系。以下从原理、关键要素、数学形式、优缺点等方面展开说明：

**一、核心问题：条件期望的非参数估计**

回归分析的目标是估计给定 X=x 时 Y 的条件期望 m(x)=E[Y∣X=x]。传统参数回归（如线性回归）假设 m(x) 符合某种已知函数形式（如 m(x)=β0+β1x），并通过最小化误差平方和求解参数 β。
 核回归则​**​不做函数形式假设​**​，直接利用数据本身的结构估计 m(x)，适用于未知或复杂非线性关系的场景。

**二、核心思想：局部加权平均**

核回归的核心是“**局部性**”：对于预测点 x，仅关注训练数据中与 x 邻近的样本点，并根据距离赋予不同权重（距离越近，权重越高），最终通过加权平均得到 m(x) 的估计值。
 这一思想可通过​**​核函数​**​（Kernel Function）实现，核函数量化了样本点与预测点的“邻近程度”。

**三、关键要素：核函数与带宽**

1. 核函数（Kernel Function）

核函数 K(⋅) 是一个对称的非负函数，满足：

- K(u)≥0（非负性）；
- ∫−∞+∞K(u)du=1（归一性，保证权重总和为1）；
- 通常关于 u=0 对称（K(−u)=K(u)），距离越近（u 越小），核函数值越大。

常用核函数包括：

- **高斯核（Gaussian Kernel）**：K(u)=2π1e−u2/2（光滑，无界）；
- **均匀核（Uniform Kernel）**：K(u)=21⋅I(∣u∣≤1)（简单，有界）；
- **三角核（Triangular Kernel）**：K(u)=(1−∣u∣)⋅I(∣u∣≤1)（线性衰减）；
- **Epanechnikov核**：K(u)=43(1−u2)⋅I(∣u∣≤1)（最小化均方误差）。

2. 带宽（Bandwidth, h）

带宽 h 是控制邻域大小的超参数，决定了核函数的“宽度”：

- h 越大，核函数的支撑范围越广，更多远距离样本会被纳入加权平均，估计结果更平滑（偏差小，方差大）；
- h 越小，核函数仅聚焦于极邻近的样本，估计结果更灵活（偏差大，方差小），但易受噪声影响（过拟合）。

**四、数学形式：Nadaraya-Watson估计量**

核回归的最经典实现是**Nadaraya-Watson估计量**（NW估计），其公式为：
$$
m^h(x)=∑i=1nK(hx−Xi)∑i=1nK(hx−Xi)Yi
$$
其中：

- n 是训练样本量；
- Xi,Yi 是第 i 个样本的自变量和因变量值；
- h>0 是带宽；
- 分子是“加权因变量和”，分母是“加权样本和”（保证估计值的无偏性）。

**五、扩展与改进**

Nadaraya-Watson估计量是最基础的核回归方法，实际应用中常通过以下方式改进：

1. 局部线性回归（Local Linear Regression）

NW估计本质是“局部常数”加权（假设邻域内 m(x) 为常数），可能导致边界区域估计偏差。局部线性回归在邻域内拟合一个线性模型 m(x)≈β0+β1(Xi−x)，通过最小化加权残差平方和求解 β0（即 m^(x)），结果更稳定。

2. 自适应带宽选择

带宽 h 的选择直接影响估计效果，常用方法包括：

- **交叉验证（CV）**：选择使预测误差（如均方误差）最小的 h；
- **经验法则**：如 Silverman 经验法则（基于数据的标准差）；
- **插件法**：通过核密度估计的理论公式推导 h。

**六、优缺点分析**

优点：

- **非参数性**：无需假设 m(x) 的函数形式，适用于复杂非线性关系；
- **灵活性**：通过调整核函数和带宽，可适应不同平滑需求；
- **理论保证**：在满足一定条件下（如样本独立同分布、核函数满足光滑性），估计量具有一致性（Consistency）和渐近正态性。

缺点：

- **计算复杂度高**：对每个预测点 x 需遍历所有样本计算核权重，大数据集下效率低（可通过核技巧或近似方法优化）；
- **边界效应**：当 x 位于数据分布的边界时（如 x 远大于所有 Xi），核权重集中在少数样本，估计偏差增大；
- **对带宽敏感**：带宽选择不当（过大或过小）会导致欠拟合或过拟合；
- **解释性弱**：无法像参数模型一样提供明确的系数解释。

**总结**

核回归通过核函数的局部加权机制，实现了对复杂非线性关系的灵活估计，是理解非参数统计的基础工具。其核心是平衡“偏差”与“方差”（通过调整带宽），并在实际应用中常与其他方法（如局部线性回归）结合以提升性能。