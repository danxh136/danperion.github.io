---
layout: article
title: "回归模型-1多元线性回归"
---

####  1.多元线性回归

| 看模型的范式 |         多元线性回归         |
| :----------: | :--------------------------: |
|  问题是什么  |         回归拟合问题         |
|  模型是什么  |           线性模型           |
|   优化指标   | 最小二乘损失，最小化均方误差 |
|   求解方法   |          最小二乘法          |
|   评价模型   |            $R^2$             |

当对总体中 $n$ 个个体观测时候, 记第 $j$ 次观测样本的解释变量为$ x_{j1}, x_{j2}, …, x_{j(p-1)}$， 而响应变量记为 $y_{j} = [y_{j1}, y_{j2}, …, y_{jm}]^T$，$ j = 1, …, n$. 使用矩阵表达, 则
$$
{\bf Y}_{n \times m} = \left[ \begin{array}{cccc} y_{11} & y_{12} & \cdots & y_{1m} \\ y_{21} & y_{22} & \cdots & y_{2m} \\ \vdots & & & \vdots \\ y_{n1} & y_{n2} & \cdots & y_{nm} \end{array} \right] = [ {\bf y}_{(1)}, {\bf y}_{(2)}, \cdots, {\bf y}_{(m)} ]
$$
其中$ x_{i0} ≡ 1$，$i = 1, …, n.$

当响应变量为多元时候，不妨设$ m $个响应变量，$Y_{1}, \ldots, Y_{m}$， 解释变量为 $x_{1}, \ldots, x_{p-1}$, 考虑解释变量与响应变量之间的关系，假设有如下总体回归模型:

$$
Y_{1} = \beta_{01} + \beta_{11}x_{1} + \cdots + \beta_{(p-1)1}x_{p-1} + \epsilon_{1}​
\\
Y_{2} = \beta_{02} + \beta_{12}x_{1} + \cdots + \beta_{(p-1)2}x_{p-1} + \epsilon_{2}​
\\
\vdots​
\\
Y_{m} = \beta_{0m} + \beta_{1m}x_{1} + \cdots + \beta_{(p-1)m}x_{p-1} + \epsilon_{m}​
$$
也就是假设每个指标 $Y_{i} $和解释变量之间存在线性关系。误差项$ e = [\epsilon_{1}, \epsilon_{2}, \ldots, \epsilon_{m}]^T $满足假设$E(\mathbf{\epsilon}) = 0$，$\mathrm{Cov}(\mathbf{\epsilon}) = \Sigma = (\sigma_{ij})$

记
$$
B_{p \times m} = \left[\begin{array}{cccc}
\beta_{01} & \beta_{02} & \cdots & \beta_{0m} \\
\beta_{11} & \beta_{12} & \cdots & \beta_{1m} \\
\vdots & \vdots & \ddots & \vdots \\
\beta_{(p-1)1} & \beta_{(p-1)2} & \cdots & \beta_{(p-1)m}
\end{array}\right] = \left[\beta_1, \beta_2, \cdots, \beta_m\right]
$$

$$
\epsilon_{n \times m} = \left[\begin{array}{cccc}
\epsilon_{11} & \epsilon_{12} & \cdots & \epsilon_{1m} \\
\epsilon_{21} & \epsilon_{22} & \cdots & \epsilon_{2m} \\
\vdots & \vdots & \ddots & \vdots \\
\epsilon_{n1} & \epsilon_{n2} & \cdots & \epsilon_{nm}
\end{array}\right] = \left[\epsilon_1, \epsilon_2, \cdots, \epsilon_m\right] = \left(\begin{array}{c}
\epsilon_{1}^{\prime} \\
\epsilon_{2}^{\prime} \\
\vdots \\
\epsilon_{n}^{\prime}
\end{array}\right)
$$

从而，**多元线性回归模型**(Multivariate linear model) 的矩阵表达:
$$
\mathbf{Y}_{n \times m} = X_{n \times p} B_{p \times m} + \epsilon_{n \times m}= \left[X \beta_1, \cdots, X \beta_m\right] + \left[\epsilon_1, \epsilon_2, \cdots, \epsilon_m\right]
$$

其中$E\epsilon_i=0$，$Cov(\epsilon_i,\epsilon_j)=\sigma_{ij}I_{n}$，$i,j=1,2,\ldots,m$，虽然对第$k$个观测的测量误差$\epsilon_k$有协方差矩阵$\Sigma$，但对不同个体的观测值不相关。
从上面的模型中可以看出对第$i$个响应$\mathbf{y}_{(i)}$，其服从线性回归模型
$$
\mathbf{y}_{(i)}=X\beta_i+\epsilon_i
$$
其中$E\epsilon_i=0$，$Cov(\epsilon_i)=\sigma_{ii}I_{n}$，$i=1,\ldots,m$

- 对第$i$个响应变量来说，$n$次观测之间不相关

- 不同响应变量的观测之间存在相关

基于第$i$个响应变量$\mathbf{y}_{(i)}$的回归模型可得$\beta_i$的最小二乘估计

$$
\hat\beta_{(i)}=(X^TX)^{-1}X'\mathbf{y}_{(i)}
$$
将这些估计量放在一起组成矩阵，我们有
$$
\begin{aligned}
\hat{B}&=[\hat{\beta}_{(1)},\hat{\beta}_{(2)},\ldots,\hat{\beta}_{(m)}]
\\&=(X^TX)^{-1}X^T[\mathbf{y}_{(1)},\mathbf{y}_{(2)},\cdots,\mathbf{y}_{(m)}]
\\&=(X^TX)^{-1}X^T\mathbf{Y}
\end{aligned}
$$
参考：

多元回归  课件http://staff.ustc.edu.cn/~zwp/

周志华 - 2016 - 机器学习 Machine learning

概率论与数理统计 第4版 浙江大学 盛骤等

代码示例：

```python
import numpy as np
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # 用于三维绘图

plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号

# 设置随机种子保证可复现性
np.random.seed(42)

# 生成二维特征数据（x1和x2）
# x1范围：0-10（比原数据范围更大），生成50个均匀分布的随机数
x1 = np.random.uniform(low=0, high=10, size=50)
# x2范围：0-5（与原数据范围接近），生成50个均匀分布的随机数
x2 = np.random.uniform(low=0, high=5, size=50)

# 组合成特征矩阵x（形状：50行2列）
x = np.column_stack((x1, x2))

# 生成目标值y（模拟真实线性关系 + 噪声）
# 真实关系假设为：y = 2.5*x1 + 1.8*x2 + 2（接近原数据的隐含关系）
# 添加正态分布噪声（均值0，标准差2），使数据更真实
noise = np.random.normal(loc=0, scale=2, size=50)
y = 2.5 * x1 + 1.8 * x2 + 2 + noise

reg = LinearRegression()
reg.fit(x, y)  # 用扩展数据训练模型


print(f"模型权重（系数）: {reg.coef_.round(4)}")       # 输出x1和x2的系数
print(f"模型阈值（截距）: {reg.intercept_.round(4)}")  # 输出常数项

# 创建网格数据用于绘制回归平面（覆盖特征的实际范围）
x1_min, x1_max = x1.min(), x1.max()
x2_min, x2_max = x2.min(), x2.max()
x1_plot = np.linspace(x1_min, x1_max, 30)  # x1方向的30个点
x2_plot = np.linspace(x2_min, x2_max, 30)  # x2方向的30个点
X1, X2 = np.meshgrid(x1_plot, x2_plot)     # 生成网格矩阵

# 计算回归平面的预测值（基于模型参数）
Y_pred = reg.predict(np.column_stack((X1.ravel(), X2.ravel()))).reshape(X1.shape)

# 创建三维画布
fig = plt.figure(figsize=(12, 8))
ax = fig.add_subplot(111, projection='3d')

# 绘制原始数据点（红色散点）
ax.scatter(x1, x2, y, c='r', s=40, marker='o', label='原始数据点')

# 绘制回归平面（半透明蓝色）
ax.plot_surface(X1, X2, Y_pred, alpha=0.5, cmap='Blues', label='拟合平面')

# 设置坐标轴标签和标题
ax.set_xlabel('特征X1', fontsize=12)
ax.set_ylabel('特征X2', fontsize=12)
ax.set_zlabel('目标值Y', fontsize=12)
ax.set_title('多元线性回归拟合效果（扩展数据）', fontsize=14)

# 添加图例（注意：3D图中图例需要手动创建代理对象）
from matplotlib.lines import Line2D
legend_elements = [
    Line2D([0], [0], marker='o', color='w', markerfacecolor='r', markersize=10, label='原始数据'),
    Line2D([0], [0], color='blue', lw=4, alpha=0.5, label='拟合平面')
]
ax.legend(handles=legend_elements, loc='upper right')

# 调整视角（可选，根据需要修改azim和elev参数）
ax.view_init(elev=20, azim=-45)

plt.show()  # 显示图像
```

